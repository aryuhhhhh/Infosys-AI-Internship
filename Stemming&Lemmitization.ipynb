{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "7gRLDpEAPrr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByyBVzb_Pi0k",
        "outputId": "2c8b2ef8-8e02-4611-ff82-b2df3313d63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words:  ['runing', 'showing', 'cuting', 'visibility']\n",
            "Stem words:  ['rune', 'show', 'cute', 'visibl']\n"
          ]
        }
      ],
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# example words\n",
        "words = ['runing','showing','cuting','visibility']\n",
        "\n",
        "# applying stemmer on each words\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words: \",words)\n",
        "print(\"Stem words: \", stemmed_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnuFqjfMPrUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [\n",
        "    \"\"\"Good morning, team. \\n Today’s focus is on the data ingestion bottleneck and the stemming cleaning workflow validation.\\n Let’s drive alignment and carve out next steps.\"\"\",\n",
        "\n",
        "\"\"\"Morning. \\n I’ve completed the baseline preprocessing evaluation and created a random text dataset for benchmarking. \\n Ready to walk through findings.\"\"\"\n",
        "]\n",
        "\n",
        "with open(\"file.txt\",'w') as f:\n",
        "  f.writelines(lst)\n",
        "\n",
        "with open(\"file.txt\",'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcycTq7IPrRf",
        "outputId": "73929c61-32a0-4dd0-98a6-308c50857fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good morning, team. \n",
            " Today’s focus is on the data ingestion bottleneck and the stemming/cleaning workflow validation.\n",
            " Let’s drive alignment and carve out next steps.Morning. \n",
            " I’ve completed the baseline preprocessing evaluation and created a random text dataset for benchmarking. \n",
            " Ready to walk through findings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlMXUdAdTeW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "\n",
        "with open('file.txt','r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "with open(\"stemme_words.txt\",'w') as f:\n",
        "  f.write(\" \".join(stemmed_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZhiyoRITeTj",
        "outputId": "f5fbf9f1-3710-45a7-fcfd-150caeeb1f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAhFPtU5TeQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVHsi9tVTeN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJ8uR7p0TeLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmitization**"
      ],
      "metadata": {
        "id": "UvDjc7VgQadO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['rocks','better','cuting','corpora']\n",
        "\n",
        "lemmatize_words = [lemmatizer.lemmatize(word,pos='a') for word in words]\n",
        "\n",
        "print(\"Original words: \",words)\n",
        "print(\"Lemmatize words: \", lemmatize_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDhrIHJ6PrOr",
        "outputId": "3bad29cc-a4f7-4fa5-92ab-40883f7917f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words:  ['rocks', 'better', 'cuting', 'corpora']\n",
            "Lemmatize words:  ['rocks', 'good', 'cuting', 'corpora']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBARblR9PrLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem  import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')        # You MUST add this for tokenization\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "with open(\"file.txt\",'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "lemmatize_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "with open(\"lemmetizer_word.txt\",'w') as f:\n",
        "  f.write(\" \".join(lemmatize_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOXH-PkiPrIr",
        "outputId": "f467c3c8-04b2-4b47-8f77-b671f5f26694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsoGYTmWPrFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJYuz0TOPrC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}